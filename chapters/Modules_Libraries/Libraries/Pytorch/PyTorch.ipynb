{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udd01 Add LSTM Network for GenAI (e.g., Text Classification)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "\n", "class LSTMGenAI(nn.Module):\n", "    def __init__(self, input_size, hidden_size, output_size):\n", "        super(LSTMGenAI, self).__init__()\n", "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n", "        self.fc = nn.Linear(hidden_size, output_size)\n", "\n", "    def forward(self, x):\n", "        out, _ = self.lstm(x)\n", "        out = self.fc(out[:, -1, :])\n", "        return out\n", "\n", "# Dummy input: 16 samples, 5 time steps, 10 features\n", "sequence_input = torch.rand(16, 5, 10)\n", "lstm_model = LSTMGenAI(10, 20, 2)\n", "output = lstm_model(sequence_input)\n", "print(output.shape)  # Expected: [16, 2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \u2728 Transformer Layer Example (Simplified Version)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["transformer_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n", "transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=6)\n", "\n", "# Dummy input: sequence length 10, batch size 16, feature size 512\n", "src = torch.rand(10, 16, 512)\n", "out = transformer_encoder(src)\n", "print(out.shape)  # Expected: [10, 16, 512]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd17 Hugging Face Transformers Integration (Text Generation)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !pip install transformers\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "\n", "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n", "model = AutoModelForCausalLM.from_pretrained('gpt2')\n", "\n", "input_text = \"Generative AI is\"\n", "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n", "\n", "output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n", "print(tokenizer.decode(output[0], skip_special_tokens=True))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}